\documentclass{beamer}

\usepackage{graphicx}

\input{../../../../texmacros/commands.tex}

\usetheme{Madrid}

\DeclareMathOperator{\RSS}{RSS}
\DeclareMathOperator*{\Argmin}{argmin}

\begin{document}
    
\setlength{\parskip}{1em}
\begin{frame}
    \title{Tree-based methods for classification and regression}
    \date{DATA 607 --- Session 3 --- 04/03/2019}
    \maketitle
\end{frame}


\begin{frame}{Regression trees}
    Dataset: $(\vx_1,y_1), (\vx_2, y_2),\ldots, (\vx_n, \vy_n)$, where $x_i\in R\subseteq\RR^p$ and $y_i\in\RR$

    \textbf{Regression tree construction:}
    \begin{itemize}
        \item Divide the predictor space, $R$, into disjoint regions, $R_1,\ldots,R_J$
        \item If a new observation, $\vx$, falls into region $R_j$, set
        $\hat r(\vx)$ equal to the average $y_i$ such that $x_i\in R_j$.
    \end{itemize}

    How do we subdivide the predictor space? \textbf{recursive bisection}

    Given an index $j$ and a \emph{cutpoint} $s\in\RR$,
    bisect $R$ into two pieces:
    \[
        R_{0} = \{\vx\in R : x_{j} \leq s\},\qquad
        R_{1} = \{\vx\in R : x_{j} > s\}
    \]

    Then bisect $R_{0}$ and $R_{1}$...
\end{frame}

\begin{frame}{}
    Given an index $j_{0}$ and a cutpoint $s_{0}\in\RR$,
    bisect $R_{00}$ into two pieces:
    \[
        R_{00} = \{\vx\in R_{0} : x_{j_{0}} \leq s_{0}\},\qquad
        R_{01} = \{\vx\in R_{0} : x_{j_{0}} > s_{0}\}.
    \]

    Given an index $j_{1}$ and a cutpoint $s_{1}\in\RR$,
    bisect $R_{01}$ into two pieces:
    \[
        R_{10} = \{\vx\in R_{1} : x_{j_{1}} \leq s_{1}\},\qquad
        R_{11} = \{\vx\in R_{1} : x_{j_{1}} > s_{1}\}
    \]

    Then bisect $R_{00}$, $R_{01}$, $R_{01}$, and $R_{10}$...

    Etc...
\end{frame}

\begin{frame}{}
    \textbf{Questions:}
    \begin{enumerate}
        \item How do we find the indices $j$, $j_0$, $j_1$, $\ldots$
        and the cutpoints $s$, $s_0$, $s_1$, $\ldots$?
        \item When do we stop?
    \end{enumerate}

    \textbf{Answers:}
    \begin{enumerate}
        \item For each pair $(j, s)$, let $\Var^-(j, s)$ and $\Var^+(j, s)$ be the variances of
        \[
            \{y_i : x_{i,j} \leq s\}\quad\text{and}\quad
            \{y_i : x_{i,j} > s\},
        \]
        respectively.
    
        Take $(j, s)$ to be the pair that \emph{minimizes the total variance},
        \[
            \Var^-(j, s) + \Var^+(j, s).
        \]
    \end{enumerate}
\end{frame}

\begin{frame}{}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item Don't bisect a region if the number of training points it contains is smaller than some threshold.
    \end{enumerate}
\end{frame}

\begin{frame}{Classification trees}
    Dataset $(\vx_1,y_1), (\vx_2, y_2),\ldots, (\vx_n, \vy_n)$, where 
    \[
    x_i\in R\subseteq\RR^p,\quad y_i\in\{1,\ldots,K\}.
    \]
    \textbf{Classification tree construction:}
    \begin{itemize}
        \item Divide the predictor space, $R$, into disjoint regions, $R_1,\ldots,R_J$
        \item If a new observation, $\vx$, falls into region $R_j$, set
        $\hat r(\vx)$ equal to most common class label, $y_i$, for which $x_i\in R_j$.
    \end{itemize}

    Partition $R$ via the same recusrive bisection procedure described for
    regression trees, except we replace the variance minimization criterion
    with one more suited to categorical response variables.
\end{frame}

\begin{frame}{}
    To bisect a region, $R$:

    For class labels $k$, indices $j$, and cutpoints $s$, let
    \begin{align*}
        \hat p_k(j, s) &= \frac{|\{i : x_i\in R,\, y_i=k\}|}{|\{i : x_i\in R\}|}\\
        G(j, s) &= \sum_{k=1}^K \hat p_k(j, s)(1 - \hat p_k(j, s)) &&\text{(Gini index)}\\
        D(j, s) &= -\sum_{k=1}^K \hat p_k(j, s)\log \hat p_k(j, s) &&\text{(cross entropy)}
    \end{align*}

    Choose $(j, s)$ to minimize either $G(j, s)$ or $D(j, s)$.

    Gini index and cross entropy are examples of \emph{(im)purity measures}.
\end{frame}

\begin{frame}{Feature importance}
Consider all the regions that split based on the $j$-th feature:
\[
    R_1,R_2,\ldots,R_{m_j}
\]
Let
\[
 I_k = \{i : x_i\in R_k\},\quad \bar y_k = \frac1{|I_k|}\sum_{i\in I_k} y,
 \quad SS_k = \sum_{i:x_i\in R_k}(y_i - \bar y_k)^2
 \]
Similarly, let $SS_k'$ and $SS_k''$ be the sums of squares associated to the regions, $R_k'$ and $R_k''$, that $R_k$ is split into.
Then
\[
    \Delta_{j, k} := SS_k - SS_k' - SS_k'' \geq 0.
\]
The absolute and relative importance of feature $j$ are:
\[
\Delta_j = \sum_{k=1}^{u_j} \Delta_{j, k},\qquad \delta_j = \frac{\Delta_j}{\sum_{j=1}^p \Delta_j}
\]
\end{frame}

\end{document}