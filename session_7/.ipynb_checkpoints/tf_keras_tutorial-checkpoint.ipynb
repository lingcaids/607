{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential networks; layers\n",
    "- A **feed-forward** or **sequential** neural network is constructed using the `tf.keras.Sequential` class construtor.\n",
    "It takes a list of **layers** as input. The last layer in the list is the output layer. The rest are hidden layers.\n",
    "- Layers are defined in the `tf.keras.layers` module.\n",
    "- In a **dense** layer, a unit is connected with every unit in the previous layer.\n",
    "- The first layer in a sequential network is responsible for specifying the dimension of the data, i.e., the number of columns of the data matrix, in the `input_dim` keyword argument.\n",
    "- A sequential model stores its list of layers in its `layers` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: [<tensorflow.python.keras.layers.core.Dense object at 0x1a2b354a58>, <tensorflow.python.keras.layers.core.Dense object at 0x1a2b354cf8>]\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, input_dim=2, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "print(f\"Layers: {model.layers}\")\n",
    "\n",
    "# Alternatively\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(3, input_dim=2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "- Weights associated to connections between units are initialized randomly. Biases are initialized to zero.\n",
    "- Inspect the weights using the `get_weights` method.\n",
    "- It returns a list of length $2L$, $L$ being the number of layers, containing the weight matrices and bias vectors of each layer.\n",
    "- The weight matrix of dense layer $\\ell$ has size $p_{\\ell - 1}\\times p_\\ell$, where $p_\\ell$ is the number of units in the $\\ell$-th layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00515556,  0.8186511 , -0.23828983],\n",
      "       [-0.5057989 , -0.7630539 ,  0.18627644]], dtype=float32), array([0., 0., 0.], dtype=float32), array([[ 0.97376835],\n",
      "       [ 1.2173446 ],\n",
      "       [-0.31684196]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.97376835],\n",
       "       [ 1.2173446 ],\n",
       "       [-0.31684196]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, b1, w2, b2 = model.get_weights()\n",
    "\n",
    "# model.set_weights([np.zeros_like(w1), b1, np.zeros_like(w2), b2])\n",
    "# model.set_weights([w1, np.ones_like(b1), w2, np.ones_like(b2)])\n",
    "\n",
    "print(model.get_weights())\n",
    "w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation; prediction\n",
    "- If $W^{[\\ell]}$, $b^{[\\ell]}$, and $A_\\ell$ are the weight matrix, and bias vector and activation function of layer $\\ell$ and $a^{[\\ell]}$ and $a^{[\\ell-1]}$ is the vector of activations of layers $\\ell$ and $\\ell - 1$, then $$a^{[\\ell]} = A_\\ell(a^{[\\ell - 1]}W^{[\\ell]} + b^{[\\ell]}).$$\n",
    "- We define the activations of the input layer to be the input values, themselves.\n",
    "- To propagate data through a network, use the `predict` method.\n",
    "- To check our understanding, we can do this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5299989 ]\n",
      " [0.5       ]\n",
      " [0.4889929 ]\n",
      " [0.62795687]\n",
      " [0.4726238 ]\n",
      " [0.49364573]\n",
      " [0.46744058]\n",
      " [0.93751174]\n",
      " [0.4778661 ]\n",
      " [0.48844033]]\n",
      "[[0.52999889]\n",
      " [0.5       ]\n",
      " [0.48899289]\n",
      " [0.62795686]\n",
      " [0.47262379]\n",
      " [0.49364572]\n",
      " [0.46744056]\n",
      " [0.93751179]\n",
      " [0.47786611]\n",
      " [0.48844033]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(size=(10, 2))\n",
    "print(model.predict(X))\n",
    "\n",
    "relu = tf.keras.activations.relu\n",
    "sigmoid = tf.keras.activations.sigmoid\n",
    "\n",
    "w1, b1, w2, b2 = model.get_weights()\n",
    "a0 = X\n",
    "a1 = relu(a0.dot(w1) + b1).numpy()\n",
    "a2 = sigmoid(a1.dot(w2) + b2).numpy()\n",
    "\n",
    "print(a2)\n",
    "# print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Encode this 1-layer neural network with sigmoid activation as a Keras sequential model. Set the weights and bias as indicated.\n",
    "- Predict the values for $(0, 0)$, $(1, 0)$,  $(0, 1)$, and $(1, 1)$.\n",
    "\n",
    "<img src=\"one_and_three_nn.png\" height=\"50%\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[9.9995458e-01]\n",
      " [4.5397872e-05]\n",
      " [4.5397872e-05]\n",
      " [9.3576236e-14]]\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, input_dim=2, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "my_weights = [np.array([[-20], [-20]]), np.array([10])]\n",
    "model.set_weights(my_weights)\n",
    "model.get_weights()\n",
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "print(model.predict(X))\n",
    "print(model.predict_classes(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Encode this 2-layer neural network as a Keras sequential model. Set the weights and bias as indicated.\n",
    "- Predict the values for $(0, 0)$, $(1, 0)$,  $(0, 1)$, and $(1, 1)$.\n",
    "\n",
    "<img src=\"xnor.png\" height=\"50%\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- In practice we don't assign weights; the network **learns** the weights from **training data**.\n",
    "- Given a data point $(\\boldsymbol{x}, y)$, can feed $\\boldsymbol{x}$ into the network which returns a quantity $\\widehat y$.\n",
    "- Error in approimating of $y$ by $\\widehat y$ is encoded in a **loss function** $L(y, \\widehat{y})$.\n",
    "- Which loss function to use is part of the design of your machine learning algorithm. It is not intrinsic to the neural network; you must provide it to the `compile` method.\n",
    "- By **backpropagating** this error through the network, `tensorflow` systematically adjusts the weights and biases to reduce the error.\n",
    "Specity the **optimizer**, the numerical algorithm used to adjust the weights and biases, as the `optimizer` argument of the model's `compile` method.\n",
    "- A training loop can makes multiple passes over the training date. Each pass through an **epoch**. Specify the number of training epochs to the `compile` method as the `epochs` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, input_dim=2, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "X = np.random.normal(size=(1, 2))\n",
    "y = np.random.randint(2, size=(1,))\n",
    "print(X, y, \"\\n\")\n",
    "\n",
    "y_p = model.predict(X)\n",
    "\n",
    "loss_should_be = tf.keras.losses.binary_crossentropy(y, y_p)\n",
    "print(f\"The cross entropy loss should be: {loss_should_be}\\n\")\n",
    "\n",
    "h = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(size=(20, 2))\n",
    "y = np.random.randint(2, size=(20,))\n",
    "expected_training_accuracy = np.sum(model.predict_classes(X).reshape(-1) == y)/len(y)\n",
    "print(f\"Expected training accuracy: {expected_training_accuracy}\\n\")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 10000\n",
    "n_test = 1000\n",
    "X_train = np.random.randint(2, size=(n_train, 2))\n",
    "X_test = np.random.randint(2, size=(n_test, 2))\n",
    "y_train = np.logical_not(np.logical_xor(X_train[:, 0], X_train[:, 1]))*1\n",
    "y_test = np.logical_not(np.logical_xor(X_test[:, 0], X_test[:, 1]))*1\n",
    "X_train = X_train + np.random.normal(0, 0.2, size=(n_train, 2))\n",
    "X_test = X_test + np.random.normal(0, 0.2, size=(n_test, 2))\n",
    "plt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], 'ro')\n",
    "plt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], 'bo')\n",
    "plt.show()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, input_dim=2, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model.predict_classes(X_test).reshape(-1)\n",
    "sum(y_p == y_test)/len(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.random.uniform(size=(1000, 2))\n",
    "v = model.predict_classes(U).reshape(-1)\n",
    "plt.plot(U[v == 0, 0], U[v == 0, 1], 'ro')\n",
    "plt.plot(U[v == 1, 0], U[v == 1, 1], 'bo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Build a network that can recognize this pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(30), h[\"val_loss\"], range(30), h[\"loss\"], range(30),  h[\"val_accuracy\"], range(30), h[\"accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Construct a sequential neural network that can learn the pattern depicted below.\n",
    "- Experiment with numbers of layers, numbers of units in each layer, optimizer (`optimizer=\"rmsprop\"`) typically works well), etc.\n",
    "- Note any interesting misclassification patterns you find. \n",
    "\n",
    "<img src=\"boxed_circle.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to generate data like this:\n",
    "n = 2**10\n",
    "X = np.random.uniform(size=(n, 2))\n",
    "y = np.linalg.norm(X - np.array([0.5, 0.5]), axis=1) < 0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
